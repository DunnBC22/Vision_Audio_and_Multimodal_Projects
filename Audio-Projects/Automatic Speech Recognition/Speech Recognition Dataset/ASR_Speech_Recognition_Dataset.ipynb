{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition\n",
    "\n",
    "Dataset Source: https://www.kaggle.com/datasets/phmanhth/speech-recognition-dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, glob\n",
    "os.environ['TOKENIZERS_PARALLELISM']='false'\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict, Audio, load_dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModelForCTC, TrainingArguments\n",
    "from transformers import AutoProcessor, set_seed, Trainer\n",
    "\n",
    "import evaluate\n",
    "\n",
    "!git lfs install"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Library Verisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language/Library | Version\n",
      "---------------- | --------\n",
      "          Python : 3.9.12\n",
      "           NumPy : 1.24.3\n",
      "          Pandas : 2.0.1\n",
      "           Torch : 2.0.0\n",
      "        Datasets : 2.11.0\n",
      "    Transformers : 4.27.4\n",
      "        Evaluate : 0.4.0\n"
     ]
    }
   ],
   "source": [
    "n = 18\n",
    "\n",
    "print(f\"Language/Library\".rjust(n-2), '|', 'Version')\n",
    "print('-' * (n-2), '|', '--------')\n",
    "print(\"Python :\".rjust(n), sys.version[0:6])\n",
    "print(\"NumPy :\".rjust(n), np.__version__)\n",
    "print(\"Pandas :\".rjust(n), pd.__version__)\n",
    "print(\"Torch :\".rjust(n), torch.__version__)\n",
    "print(\"Datasets :\".rjust(n), datasets.__version__)\n",
    "print(\"Transformers :\".rjust(n), transformers.__version__)\n",
    "print(\"Evaluate :\".rjust(n), evaluate.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingest Dataset Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 22075 entries, 0 to 40\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   file_name      22075 non-null  object\n",
      " 1   transcription  22075 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 517.4+ KB\n"
     ]
    }
   ],
   "source": [
    "parent_dir = \"/Users/briandunn/Desktop/Audio_Projects/Audio Datasets/ASR - Speech Recognition Dataset/data\"\n",
    "areas = ['Health & Lifestyle', 'Science & Technology']\n",
    "\n",
    "text_df = pd.DataFrame()\n",
    "file_names = []\n",
    "\n",
    "# list of image paths\n",
    "for area in areas:\n",
    "    folder_to_search = os.path.join(parent_dir, \n",
    "                                    area, \n",
    "                                    \"*\", \n",
    "                                    \"metadata.txt\")\n",
    "    \n",
    "    temp_list = glob.glob(folder_to_search)\n",
    "    file_names = file_names + temp_list\n",
    "\n",
    "for file in file_names:\n",
    "    temp_df = pd.read_csv(file,\n",
    "                          sep='|',\n",
    "                      engine='c',\n",
    "                      names=[\"file_name\", \"transcription\"])\n",
    "    \n",
    "    temp_df['file_name'] = file.split(\"metadata\")[0]\\\n",
    "        .split(\"/ASR - Speech Recognition Dataset/data/\")[-1] + \\\n",
    "        \"wavs/\" + \\\n",
    "        temp_df['file_name'] + \\\n",
    "        \".wav\"\n",
    "    \n",
    "    text_df = pd.concat([text_df, temp_df])\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Save data to file\n",
    "folder_location_to_save_input_file = \"/Users/briandunn/Desktop/Audio_Projects/Audio Datasets/ASR - Speech Recognition Dataset\"\n",
    "\n",
    "text_df.to_csv(os.path.join(folder_location_to_save_input_file,\n",
    "                            \"data\",\n",
    "                            \"metadata.csv\"), \n",
    "              index=False)\n",
    "\n",
    "text_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingest Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e296dbf80e4a608fedadfe4e84a40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/22566 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset audiofolder/ASR - Speech Recognition Dataset to /Users/briandunn/.cache/huggingface/datasets/audiofolder/ASR - Speech Recognition Dataset-94c4bf3175bdaafa/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b1b8ba8f194786810c2e04ebeff6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/22076 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0dee368a87b4609957a37419d28736d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/490 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2f2639b1ce407987395651829809ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/490 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f33b3b2d1fb4d1f94b874ba3e1aa5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset audiofolder downloaded and prepared to /Users/briandunn/.cache/huggingface/datasets/audiofolder/ASR - Speech Recognition Dataset-94c4bf3175bdaafa/0.0.0/6cbdd16f8688354c63b4e2a36e1585d05de285023ee6443ffd71c4182055c0fc. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4a7dc938c348448c93e585eed14509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'transcription'],\n",
       "        num_rows: 22075\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_data = load_dataset(folder_location_to_save_input_file)\n",
    "\n",
    "audio_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data = audio_data.cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Dataset into Training & Evaluation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Shape: (15452, 2)\n",
      "Testing Dataset Shape: (6623, 2)\n"
     ]
    }
   ],
   "source": [
    "audio_data = audio_data['train'].train_test_split(train_size=0.70)\n",
    "\n",
    "print(\"Training Dataset Shape:\", audio_data['train'].shape)\n",
    "print(\"Testing Dataset Shape:\", audio_data['test'].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Values/Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CKPT = \"facebook/wav2vec2-base\"\n",
    "MODEL_NAME = MODEL_CKPT.split(\"/\")[-1] + \"-Speech_Recognition_Dataset\"\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "LR = 1e-5\n",
    "\n",
    "WARMUP_STEPS = 500\n",
    "MAX_TRAINING_STEPS = 2000\n",
    "\n",
    "LOGGING_STEPS = 50\n",
    "STEPS = 1000\n",
    "\n",
    "GRAD_ACC_STEPS = 2\n",
    "STRATEGY = \"steps\"\n",
    "\n",
    "REPORTS_TO = \"tensorboard\"\n",
    "DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briandunn/Documents/deep_learning/dl/lib/python3.9/site-packages/transformers/configuration_utils.py:379: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL_CKPT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Transcription to Uppercase Letter Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9a6e6f1a2e423d97411fc4bee7ac22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15452 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b493d9053ac45809bcef18be3c7a51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6623 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_to_uppercase(example):\n",
    "    return {\"transcription\": example[\"transcription\"].upper()}\n",
    "\n",
    "audio_data = audio_data.map(convert_to_uppercase)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare/Encode Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ad7b63059c47fc9b42822a64a78ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/15452 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d527008eb44ac6bfeb05db4a95ec65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/6623 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    batch = processor(audio[\"array\"], \n",
    "                      sampling_rate=audio[\"sampling_rate\"], \n",
    "                      text=batch[\"transcription\"])\n",
    "    \n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"][0])\n",
    "    return batch\n",
    "\n",
    "encoded_audio_data = audio_data.map(prepare_dataset, \n",
    "                         remove_columns=audio_data.column_names[\"train\"], \n",
    "                         num_proc=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Compute Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds):\n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "    \n",
    "    preds_logits = preds.predictions\n",
    "    preds_ids = np.argmax(preds_logits, axis=-1)\n",
    "    \n",
    "    preds.label_ids[preds.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    \n",
    "    preds_str = processor.batch_decode(preds_ids)\n",
    "    labels_str = processor.batch_decode(preds.label_ids, group_tokens=False)\n",
    "    \n",
    "    wer = wer_metric.compute(predictions=preds_str, references=labels_str)\n",
    "    \n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForCTC: ['quantizer.weight_proj.bias', 'project_hid.bias', 'project_hid.weight', 'quantizer.codevectors', 'project_q.bias', 'quantizer.weight_proj.weight', 'project_q.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['lm_head.weight', 'lm_head.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCTC.from_pretrained(\n",
    "    MODEL_CKPT,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Data Collator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: AutoProcessor\n",
    "    padding: Union[bool, str] = \"longest\"\n",
    "    \n",
    "    def __call__(self, \n",
    "                 features: List[Dict[str, \n",
    "                                     Union[List[int], \n",
    "                                     torch.Tensor]]]\n",
    "                 ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        This function splits the inputs & labels since \n",
    "        they have to be different lengths & require \n",
    "        different padding methods.\n",
    "        \"\"\"\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"][0]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        \n",
    "        batch = self.processor.pad(input_features, \n",
    "                                   padding=self.padding, \n",
    "                                   return_tensors=\"pt\")\n",
    "        \n",
    "        labels_batch = self.processor.pad(labels=label_features, \n",
    "                                          padding=self.padding, \n",
    "                                          return_tensors=\"pt\")\n",
    "        \n",
    "        # Replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        \n",
    "        batch[\"labels\"] = labels\n",
    "        return batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate Instance of Data Collator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, \n",
    "                                           padding=\"longest\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=MODEL_NAME,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    max_steps=MAX_TRAINING_STEPS,\n",
    "    gradient_checkpointing=True,\n",
    "    learning_rate=LR,\n",
    "    logging_first_step=True,\n",
    "    logging_strategy=STRATEGY,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    save_strategy=STRATEGY,\n",
    "    save_steps=STEPS,\n",
    "    evaluation_strategy=STRATEGY,\n",
    "    eval_steps=STEPS,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=True,\n",
    "    report_to=REPORTS_TO,\n",
    "    hub_private_repo=True,\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/DunnBC22/wav2vec2-base-Speech_Recognition_Dataset into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=encoded_audio_data['train'],\n",
    "    eval_dataset=encoded_audio_data['test'],\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briandunn/Documents/deep_learning/dl/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40dc1bd81844a6083c0f7b7d2c64e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 25.3084, 'learning_rate': 2e-08, 'epoch': 0.0}\n",
      "{'loss': 37.813, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.05}\n",
      "{'loss': 32.6564, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.1}\n",
      "{'loss': 28.1036, 'learning_rate': 3e-06, 'epoch': 0.16}\n",
      "{'loss': 21.4504, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.21}\n",
      "{'loss': 0.0, 'learning_rate': 5e-06, 'epoch': 0.26}\n",
      "{'loss': 0.0, 'learning_rate': 6e-06, 'epoch': 0.31}\n",
      "{'loss': 0.0, 'learning_rate': 7e-06, 'epoch': 0.36}\n",
      "{'loss': 0.0, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.41}\n",
      "{'loss': 0.0, 'learning_rate': 9e-06, 'epoch': 0.47}\n",
      "{'loss': 0.0, 'learning_rate': 1e-05, 'epoch': 0.52}\n",
      "{'loss': 0.0, 'learning_rate': 9.666666666666667e-06, 'epoch': 0.57}\n",
      "{'loss': 0.0, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.62}\n",
      "{'loss': 0.0, 'learning_rate': 9e-06, 'epoch': 0.67}\n",
      "{'loss': 0.0, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.72}\n",
      "{'loss': 0.0, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.78}\n",
      "{'loss': 0.0, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.83}\n",
      "{'loss': 0.0, 'learning_rate': 7.666666666666667e-06, 'epoch': 0.88}\n",
      "{'loss': 0.0, 'learning_rate': 7.333333333333333e-06, 'epoch': 0.93}\n",
      "{'loss': 0.0, 'learning_rate': 7e-06, 'epoch': 0.98}\n",
      "{'loss': 0.0, 'learning_rate': 6.666666666666667e-06, 'epoch': 1.04}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b865b85e85fb4ec1a160e3f31594b87d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/828 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_wer': 1.0, 'eval_runtime': 8526.0304, 'eval_samples_per_second': 0.777, 'eval_steps_per_second': 0.097, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding files tracked by Git LFS: ['.DS_Store']. This may take a bit of time if the files are large.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 6.333333333333333e-06, 'epoch': 1.09}\n",
      "{'loss': 0.0, 'learning_rate': 6e-06, 'epoch': 1.14}\n",
      "{'loss': 0.0, 'learning_rate': 5.666666666666667e-06, 'epoch': 1.19}\n",
      "{'loss': 0.0, 'learning_rate': 5.333333333333334e-06, 'epoch': 1.24}\n",
      "{'loss': 0.0, 'learning_rate': 5e-06, 'epoch': 1.29}\n",
      "{'loss': 0.0, 'learning_rate': 4.666666666666667e-06, 'epoch': 1.35}\n",
      "{'loss': 0.0, 'learning_rate': 4.333333333333334e-06, 'epoch': 1.4}\n",
      "{'loss': 0.0, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.45}\n",
      "{'loss': 0.0, 'learning_rate': 3.6666666666666666e-06, 'epoch': 1.5}\n",
      "{'loss': 0.0, 'learning_rate': 3.3333333333333333e-06, 'epoch': 1.55}\n",
      "{'loss': 0.0, 'learning_rate': 3e-06, 'epoch': 1.6}\n",
      "{'loss': 0.0, 'learning_rate': 2.666666666666667e-06, 'epoch': 1.66}\n",
      "{'loss': 0.0, 'learning_rate': 2.3333333333333336e-06, 'epoch': 1.71}\n",
      "{'loss': 0.0, 'learning_rate': 2.0000000000000003e-06, 'epoch': 1.76}\n",
      "{'loss': 0.0, 'learning_rate': 1.6666666666666667e-06, 'epoch': 1.81}\n",
      "{'loss': 0.0, 'learning_rate': 1.3333333333333334e-06, 'epoch': 1.86}\n",
      "{'loss': 0.0, 'learning_rate': 1.0000000000000002e-06, 'epoch': 1.92}\n",
      "{'loss': 0.0, 'learning_rate': 6.666666666666667e-07, 'epoch': 1.97}\n",
      "{'loss': 0.0, 'learning_rate': 3.3333333333333335e-07, 'epoch': 2.02}\n",
      "{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 2.07}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5327093567044b58135baa3fa366542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/828 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_wer': 1.0, 'eval_runtime': 8315.7991, 'eval_samples_per_second': 0.796, 'eval_steps_per_second': 0.1, 'epoch': 2.07}\n",
      "{'train_runtime': 181071.902, 'train_samples_per_second': 0.177, 'train_steps_per_second': 0.011, 'train_loss': 2.994333236694336, 'epoch': 2.07}\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48be986c8924486b90034bc2dfdfa49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/May17_12-57-26_Brians-Mac-mini/events.out.tfevents.1684346252.Brians-Mac-mini.29839.0: 100%|#â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/DunnBC22/wav2vec2-base-Speech_Recognition_Dataset\n",
      "   36ca157..4e19e2d  main -> main\n",
      "\n",
      "To https://huggingface.co/DunnBC22/wav2vec2-base-Speech_Recognition_Dataset\n",
      "   4e19e2d..5b4b53a  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =               2.07\n",
      "  train_loss               =             2.9943\n",
      "  train_runtime            = 2 days, 2:17:51.90\n",
      "  train_samples_per_second =              0.177\n",
      "  train_steps_per_second   =              0.011\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push Model to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/DunnBC22/wav2vec2-base-Speech_Recognition_Dataset\n",
      "   5b4b53a..8700aaf  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/DunnBC22/wav2vec2-base-Speech_Recognition_Dataset/commit/8700aaf2375bc78e13a826862609b2ea8779b0d8'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8166fc650f4e4b9c6b179ad6542f23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/828 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': nan,\n",
       " 'eval_wer': 1.0,\n",
       " 'eval_runtime': 8351.8663,\n",
       " 'eval_samples_per_second': 0.793,\n",
       " 'eval_steps_per_second': 0.099,\n",
       " 'epoch': 2.07}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes & Other Takeaways\n",
    "****\n",
    "- I am curious why the loss is 0.00 after the first 250-299 steps. This is even before the 500 warmup steps conclude.\n",
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citation(s)\n",
    "\n",
    "- Model Checkpoint\n",
    "    > https://github.com/facebookresearch/fairseq/tree/main/examples/wav2vec#wav2vec-20\n",
    "    \n",
    "    > https://arxiv.org/abs/2006.11477 (wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations)\n",
    "\n",
    "- Metric (Word Error Rate [WER])\n",
    "    > @inproceedings{woodard1982, author = {Woodard, J.P. and Nelson, J.T., year = {1982}, journal = {Workshop on standardisation for speech I/O technology, Naval Air Development Center, Warminster, PA}, title = {An information theoretic measure of speech recognition performance}}\n",
    "    \n",
    "    > @inproceedings{morris2004, author = {Morris, Andrew and Maier, Viktoria and Green, Phil}, year = 2004}, month = {01}, pages = {}, title = {From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition.}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
