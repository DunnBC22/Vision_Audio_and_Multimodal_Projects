{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilabel Classification of Frames of Futurama\n",
    "\n",
    "Dataset Source: https://www.kaggle.com/datasets/gonzalorecioc/futurama-frames-with-characteronscreen-data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import PIL.Image\n",
    "from PIL import ImageDraw, ImageFont\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, ViTForImageClassification\n",
    "from transformers import TrainingArguments, Trainer, set_seed, ViTImageProcessor\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "\n",
    "!git lfs install"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Display Version of Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Python: 3.9.12\n",
      "        Pandas: 1.5.0\n",
      "         NumPy: 1.23.3\n",
      "         Torch: 1.12.1\n",
      "  Transformers: 4.26.1\n",
      "      Datasets: 2.8.0\n",
      "      Evaluate: 0.2.2\n"
     ]
    }
   ],
   "source": [
    "print(\"Python:\".rjust(15), sys.version[0:6])\n",
    "print(\"Pandas:\".rjust(15), pd.__version__)\n",
    "print(\"NumPy:\".rjust(15), np.__version__)\n",
    "print(\"Torch:\".rjust(15), torch.__version__)\n",
    "print(\"Transformers:\".rjust(15), transformers.__version__)\n",
    "print(\"Datasets:\".rjust(15), datasets.__version__)\n",
    "print(\"Evaluate:\".rjust(15), evaluate.__version__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Necessary Updates to Metadata File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>isLeela</th>\n",
       "      <th>isFry</th>\n",
       "      <th>isBender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Futurama_1_04081.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Futurama_2_17161.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Futurama_7_14497.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Futurama_6_23881.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Futurama_5_02065.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9151</th>\n",
       "      <td>Futurama_2_06193.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9152</th>\n",
       "      <td>Futurama_1_10801.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9153</th>\n",
       "      <td>Futurama_6_17593.png</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9154</th>\n",
       "      <td>Futurama_6_00313.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9155</th>\n",
       "      <td>Futurama_2_05569.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9156 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 file_name  isLeela  isFry  isBender\n",
       "0     Futurama_1_04081.png        0      1         0\n",
       "1     Futurama_2_17161.png        1      1         0\n",
       "2     Futurama_7_14497.png        1      1         1\n",
       "3     Futurama_6_23881.png        0      0         0\n",
       "4     Futurama_5_02065.png        1      0         0\n",
       "...                    ...      ...    ...       ...\n",
       "9151  Futurama_2_06193.png        0      1         0\n",
       "9152  Futurama_1_10801.png        0      0         0\n",
       "9153  Futurama_6_17593.png        1      1         1\n",
       "9154  Futurama_6_00313.png        0      1         0\n",
       "9155  Futurama_2_05569.png        0      0         1\n",
       "\n",
       "[9156 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_dir = \"/Users/briandunn/Desktop/Vit_Image_Datasets/ML CLF/Futurama Frames\"\n",
    "\n",
    "df = pd.read_csv(os.path.join(parent_dir, \"data.csv\"))\n",
    "\n",
    "df = df.rename(columns={\"file\" : \"file_name\"})\n",
    "\n",
    "df.to_csv(os.path.join(parent_dir, \"img\", \"metadata.csv\"), index=False)\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ingest Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "161bbc16e52d4f84af48ba5b3da91865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration img-00d4e2cc6bbffb36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imagefolder/img to /Users/briandunn/.cache/huggingface/datasets/imagefolder/img-00d4e2cc6bbffb36/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961a508a3b5643799a8f9413992d8091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb7ea111bb374536877d1d3aedb1bf38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a33571085f43339dfe754ab54fb71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc39d4554fb54bbea0ddab2639127fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagefolder downloaded and prepared to /Users/briandunn/.cache/huggingface/datasets/imagefolder/img-00d4e2cc6bbffb36/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=180x135>,\n",
       " 'isLeela': 0,\n",
       " 'isFry': 0,\n",
       " 'isBender': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(os.path.join(parent_dir, \"img\"), split='train')\n",
    "dataset\n",
    "dataset[12]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Dataset into Training & Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (7324, 4)\n",
      "Validation data shape: (1832, 4)\n"
     ]
    }
   ],
   "source": [
    "train_test_ds = dataset.train_test_split(test_size=0.20)\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train': train_test_ds['train'], \n",
    "    'eval': train_test_ds['test']\n",
    "    })\n",
    "\n",
    "print('Training data shape:', ds['train'].shape)\n",
    "print('Validation data shape:', ds['eval'].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define List of Labels & Label Conversion Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [x for x in df.columns if x not in ['image', 'file_name']]\n",
    "\n",
    "num_labels=len(labels)\n",
    "id2label={str(i): c for i, c in enumerate(labels)}\n",
    "label2id={c: str(i) for i, c in enumerate(labels)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Values/Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed = 42\n",
    "NUM_OF_EPOCHS = 8\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "REPORTS_TO = \"tensorboard\"\n",
    "STRATEGY = \"epoch\"\n",
    "\n",
    "MODEL_CKPT = \"google/vit-base-patch16-224\"\n",
    "MODEL_NAME = MODEL_CKPT.split(\"/\")[-1] + \"-Futurama_Image_multilabel_clf\"\n",
    "\n",
    "METRIC_NAME = \"f1\"\n",
    "DEVICE = torch.device(\"mps\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load ViT Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = ViTImageProcessor.from_pretrained(MODEL_CKPT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(sample_batch):\n",
    "    # Convert list of PIL Images into pixel values\n",
    "    inputs = feature_extractor([x for x in sample_batch['image']], return_tensors=\"pt\")\n",
    "    \n",
    "    # Prep Labels\n",
    "    labels_batch = {k: sample_batch[k] for k in sample_batch.keys() if k in labels}\n",
    "    \n",
    "    labels_matrix = np.zeros((len(sample_batch['image']), len(labels)))\n",
    "    \n",
    "    for idx, label in enumerate(labels):\n",
    "        labels_matrix[:, idx] = labels_batch[label]\n",
    "    \n",
    "    inputs[\"labels\"] = labels_matrix.tolist()\n",
    "    \n",
    "    return inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Transform Function to Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=180x135 at 0x7FE688150F40>, 'isLeela': 0, 'isFry': 0, 'isBender': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       " \n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       " \n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]]]),\n",
       " 'labels': [0.0, 0.0, 0.0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ds['train'][12])\n",
    "\n",
    "prepped_ds = ds.with_transform(transform)\n",
    "prepped_ds\n",
    "prepped_ds['train'][12]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\n",
    "    MODEL_CKPT, \n",
    "    problem_type=\"multi_label_classification\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Data Collator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(batch):\n",
    "    return {\n",
    "        'pixel_values' : torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels' : torch.tensor([x['labels'] for x in batch]),\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Function to Compute Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_metrics(predictions, \n",
    "                        labels, \n",
    "                        threshold=0.5):\n",
    "    '''\n",
    "    This function calculates & returns metrics \n",
    "    for a multilabel classification analysis.\n",
    "    '''\n",
    "    \n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    \n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    \n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    \n",
    "    preds = preds[0] if isinstance(preds, \n",
    "                    tuple) else preds\n",
    "    results = multi_label_metrics(\n",
    "                    predictions=preds, \n",
    "                    labels=labels)\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=MODEL_NAME,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=NUM_OF_EPOCHS,\n",
    "    evaluation_strategy=STRATEGY,\n",
    "    save_strategy=STRATEGY,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    disable_tqdm=False,\n",
    "    report_to=REPORTS_TO,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=METRIC_NAME,\n",
    "    logging_first_step=True,\n",
    "    hub_private_repo=True,\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/DunnBC22/vit-base-patch16-224-Futurama_Image_multilabel_clf into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepped_ds['train'],\n",
    "    eval_dataset=prepped_ds['eval'],\n",
    "    tokenizer=feature_extractor\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briandunn/Documents/nlpnn/nlp_ml/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 7324\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7328\n",
      "  Number of trainable parameters = 85800963\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4723da1b2eb24c13b2fdb6474ce1ee5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6884, 'learning_rate': 1.999727074235808e-05, 'epoch': 0.0}\n",
      "{'loss': 0.2456, 'learning_rate': 1.8635371179039304e-05, 'epoch': 0.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1832\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37367643affa4319943eafde1d49d70a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-916\n",
      "Configuration saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-916/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07229584455490112, 'eval_f1': 0.9710691823899372, 'eval_roc_auc': 0.9745990820070386, 'eval_accuracy': 0.9481441048034934, 'eval_runtime': 1680.1374, 'eval_samples_per_second': 1.09, 'eval_steps_per_second': 0.136, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-916/pytorch_model.bin\n",
      "Image processor saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-916/preprocessor_config.json\n",
      "Image processor saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/preprocessor_config.json\n",
      "Adding files tracked by Git LFS: ['.DS_Store']. This may take a bit of time if the files are large.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0749, 'learning_rate': 1.7270742358078607e-05, 'epoch': 1.09}\n",
      "{'loss': 0.0269, 'learning_rate': 1.5906113537117906e-05, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1832\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fc9a4dff164d8fa7a376ae353e8d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-1832\n",
      "Configuration saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-1832/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05449194833636284, 'eval_f1': 0.9799095931692616, 'eval_roc_auc': 0.9818209165245149, 'eval_accuracy': 0.9639737991266376, 'eval_runtime': 1758.0937, 'eval_samples_per_second': 1.042, 'eval_steps_per_second': 0.13, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-1832/pytorch_model.bin\n",
      "Image processor saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-1832/preprocessor_config.json\n",
      "Image processor saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0234, 'learning_rate': 1.4541484716157206e-05, 'epoch': 2.18}\n",
      "{'loss': 0.0086, 'learning_rate': 1.3176855895196507e-05, 'epoch': 2.73}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1832\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8626421f71f541cab9e813d5aa1872c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-2748\n",
      "Configuration saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-2748/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05799925699830055, 'eval_f1': 0.9794073329984931, 'eval_roc_auc': 0.9814291406807462, 'eval_accuracy': 0.9623362445414847, 'eval_runtime': 1649.1127, 'eval_samples_per_second': 1.111, 'eval_steps_per_second': 0.139, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-2748/pytorch_model.bin\n",
      "Image processor saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-2748/preprocessor_config.json\n",
      "Image processor saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0046, 'learning_rate': 1.181222707423581e-05, 'epoch': 3.28}\n",
      "{'loss': 0.0044, 'learning_rate': 1.044759825327511e-05, 'epoch': 3.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1832\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fddff246a4643dbaa39fb88267a29f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-3664\n",
      "Configuration saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-3664/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06122291088104248, 'eval_f1': 0.9814350225790265, 'eval_roc_auc': 0.983205248199549, 'eval_accuracy': 0.9650655021834061, 'eval_runtime': 1658.3164, 'eval_samples_per_second': 1.105, 'eval_steps_per_second': 0.138, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-3664/pytorch_model.bin\n",
      "Image processor saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-3664/preprocessor_config.json\n",
      "Image processor saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0022, 'learning_rate': 9.082969432314411e-06, 'epoch': 4.37}\n",
      "{'loss': 0.0027, 'learning_rate': 7.718340611353714e-06, 'epoch': 4.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1832\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b0e1bc6bc8428bb8c6661203743351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-4580\n",
      "Configuration saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-4580/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05915337800979614, 'eval_f1': 0.98175456135966, 'eval_roc_auc': 0.984184901660414, 'eval_accuracy': 0.9672489082969432, 'eval_runtime': 1650.3185, 'eval_samples_per_second': 1.11, 'eval_steps_per_second': 0.139, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-4580/pytorch_model.bin\n",
      "Image processor saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-4580/preprocessor_config.json\n",
      "Image processor saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0017, 'learning_rate': 6.353711790393014e-06, 'epoch': 5.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1832\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b3d23e617d4a4daefcbfe05d23b2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-5496\n",
      "Configuration saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-5496/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06344735622406006, 'eval_f1': 0.9800399201596807, 'eval_roc_auc': 0.9831794434587476, 'eval_accuracy': 0.9645196506550219, 'eval_runtime': 1655.6208, 'eval_samples_per_second': 1.107, 'eval_steps_per_second': 0.138, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-5496/pytorch_model.bin\n",
      "Image processor saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-5496/preprocessor_config.json\n",
      "Image processor saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0013, 'learning_rate': 4.989082969432315e-06, 'epoch': 6.0}\n",
      "{'loss': 0.0012, 'learning_rate': 3.624454148471616e-06, 'epoch': 6.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1832\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2681c127904d22a912f8882fa0262e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-6412\n",
      "Configuration saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-6412/config.json\n",
      "Model weights saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-6412/pytorch_model.bin\n",
      "Image processor saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-6412/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06574680656194687, 'eval_f1': 0.98173630222667, 'eval_roc_auc': 0.9839758975166857, 'eval_accuracy': 0.9667030567685589, 'eval_runtime': 1588.6633, 'eval_samples_per_second': 1.153, 'eval_steps_per_second': 0.144, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image processor saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0006, 'learning_rate': 2.259825327510917e-06, 'epoch': 7.1}\n",
      "{'loss': 0.0005, 'learning_rate': 8.951965065502185e-07, 'epoch': 7.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1832\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa533627ab5946bcbc5facee164589b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-7328\n",
      "Configuration saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-7328/config.json\n",
      "Model weights saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-7328/pytorch_model.bin\n",
      "Image processor saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-7328/preprocessor_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06680033355951309, 'eval_f1': 0.981235926945209, 'eval_roc_auc': 0.9835841216729171, 'eval_accuracy': 0.9667030567685589, 'eval_runtime': 1637.4794, 'eval_samples_per_second': 1.119, 'eval_steps_per_second': 0.14, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image processor saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/preprocessor_config.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from vit-base-patch16-224-Futurama_Image_multilabel_clf/checkpoint-4580 (score: 0.98175456135966).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 171163.2402, 'train_samples_per_second': 0.342, 'train_steps_per_second': 0.043, 'train_loss': 0.027292877175401912, 'epoch': 8.0}\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save & Log Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to vit-base-patch16-224-Futurama_Image_multilabel_clf\n",
      "Configuration saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/config.json\n",
      "Model weights saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/pytorch_model.bin\n",
      "Image processor saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/preprocessor_config.json\n",
      "Saving model checkpoint to vit-base-patch16-224-Futurama_Image_multilabel_clf\n",
      "Configuration saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/config.json\n",
      "Model weights saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/pytorch_model.bin\n",
      "Image processor saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/preprocessor_config.json\n",
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc50a42f19474268943fd471ffe250e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e61941592d4701ab150c2d9bb3b336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/DunnBC22/vit-base-patch16-224-Futurama_Image_multilabel_clf\n",
      "   699db9b..46f164c  main -> main\n",
      "\n",
      "To https://huggingface.co/DunnBC22/vit-base-patch16-224-Futurama_Image_multilabel_clf\n",
      "   46f164c..0d54221  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =                8.0\n",
      "  train_loss               =             0.0273\n",
      "  train_runtime            = 1 day, 23:32:43.24\n",
      "  train_samples_per_second =              0.342\n",
      "  train_steps_per_second   =              0.043\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate, Log, & Save Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1832\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51fb1f301b44fa0a92967da17e2e523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        8.0\n",
      "  eval_accuracy           =     0.9672\n",
      "  eval_f1                 =     0.9818\n",
      "  eval_loss               =     0.0592\n",
      "  eval_roc_auc            =     0.9842\n",
      "  eval_runtime            = 0:27:32.60\n",
      "  eval_samples_per_second =      1.109\n",
      "  eval_steps_per_second   =      0.139\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(prepped_ds['eval'])\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push Model to Hub (My Profile!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to vit-base-patch16-224-Futurama_Image_multilabel_clf\n",
      "Configuration saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/config.json\n",
      "Model weights saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/pytorch_model.bin\n",
      "Image processor saved in vit-base-patch16-224-Futurama_Image_multilabel_clf/preprocessor_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b4cd6680b6482c8553c020fb118a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/DunnBC22/vit-base-patch16-224-Futurama_Image_multilabel_clf\n",
      "   0d54221..5606d1d  main -> main\n",
      "\n",
      "To https://huggingface.co/DunnBC22/vit-base-patch16-224-Futurama_Image_multilabel_clf\n",
      "   5606d1d..b08985e  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"finetuned_from\": model.config._name_or_path,\n",
    "    \"tasks\": \"image-classification\",\n",
    "    \"tags\": ['image-classification'],\n",
    "}\n",
    "\n",
    "if args.push_to_hub:\n",
    "    trainer.push_to_hub('All DUNN!!!', **kwargs)\n",
    "else:\n",
    "    trainer.create_model_card(**kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes & Other Takeaways From This Project\n",
    "****\n",
    "- I am pleasantly surprised at how quickly this model was able to find the optimal solution.\n",
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "- Model Checkpoint\n",
    "    - @misc{wu2020visual,\n",
    "      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision}, \n",
    "      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\n",
    "      year={2020},\n",
    "      eprint={2006.03677},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.CV}\n",
    "}\n",
    "\n",
    "    - @inproceedings{deng2009imagenet,\n",
    "      title={Imagenet: A large-scale hierarchical image database},\n",
    "      author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n",
    "      booktitle={2009 IEEE conference on computer vision and pattern recognition},\n",
    "      pages={248--255},\n",
    "      year={2009},\n",
    "      organization={Ieee}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b7a754c5aaaa44a21e2f2148f50b931a5abdffe7ef71537f5954179554f39df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
