{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carvana Image Masking: Image Segmentation with LoRA\n",
    "\n",
    "Dataset Source: https://www.kaggle.com/datasets/ipythonx/carvana-image-masking-png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, glob, shutil\n",
    "os.environ['TOKENIZERS_PARALLELISM']='false'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers import AutoImageProcessor, AutoModelForSemanticSegmentation\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import peft\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Library Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Python : 3.9.12\n",
      "           NumPy : 1.25.1\n",
      "          Pandas : 2.0.3\n",
      "           Torch : 2.0.1\n",
      "    Torch Vision : 0.15.2\n",
      "    Transformers : 4.29.1\n",
      "        Evaluate : 0.4.0\n",
      "            PEFT : 0.3.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Python :\".rjust(18), sys.version[0:6])\n",
    "print(\"NumPy :\".rjust(18), np.__version__)\n",
    "print(\"Pandas :\".rjust(18), pd.__version__)\n",
    "print(\"Torch :\".rjust(18), torch.__version__)\n",
    "print(\"Torch Vision :\".rjust(18), torchvision.__version__)\n",
    "print(\"Transformers :\".rjust(18), transformers.__version__)\n",
    "print(\"Evaluate :\".rjust(18), evaluate.__version__)\n",
    "print(\"PEFT :\".rjust(18), peft.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Values/Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CKPT = \"nvidia/mit-b0\"\n",
    "MODEL_NAME = f'{MODEL_CKPT.split(\"/\")[-1]}-Image_segmentation-Carvana_Image_Masking'\n",
    "\n",
    "LR = 5e-4\n",
    "NUM_OF_EPOCHS = 10\n",
    "\n",
    "STRATEGY = \"steps\"\n",
    "REPORTS_TO = \"tensorboard\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageSegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class to create an Image \n",
    "    (Semantic) Segmentation dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, parent_dir, image_processor, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            parent_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            image_processor: image processor used to prepare images + segmentation maps.\n",
    "            train (bool): Whether to load \"training\" or \"validation\" images + annotations.\n",
    "        \"\"\"\n",
    "        self.parent_dir = parent_dir\n",
    "        self.image_processor = image_processor\n",
    "        self.train = train\n",
    "\n",
    "        sub_path = \"training\" if self.train else \"testing\"\n",
    "        self.img_dir = os.path.join(self.parent_dir, sub_path, \"images\")\n",
    "        self.ann_dir = os.path.join(self.parent_dir, sub_path, \"annotations\")\n",
    "        \n",
    "        # read images\n",
    "        image_file_names = []\n",
    "        for root, dirs, files in os.walk(self.img_dir):\n",
    "            image_file_names.extend(files)\n",
    "        self.images = sorted(image_file_names)\n",
    "        \n",
    "        # read annotations\n",
    "        annotation_file_names = []\n",
    "        for root, dirs, files in os.walk(self.ann_dir):\n",
    "            annotation_file_names.extend(files)\n",
    "        self.annotations = sorted(annotation_file_names)\n",
    "\n",
    "        assert len(self.images) == len(self.annotations), \\\n",
    "            \"There must be as many images as there are segmentation maps\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = Image.open(os.path.join(self.img_dir, self.images[idx]))\n",
    "        segmentation_map = Image.open(os.path.join(self.ann_dir, self.annotations[idx]))\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "            encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Image Processor & Ingest Training & Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "/Users/briandunn/Desktop/python_venvs/torch_cv/lib/python3.9/site-packages/transformers/models/segformer/image_processing_segformer.py:99: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(MODEL_CKPT, \n",
    "                                                     do_reduce_labels=False)\n",
    "\n",
    "training_data_folder = \"/Users/briandunn/Desktop/Image Segmentation Projects/Carvana Image Masking/data\"\n",
    "\n",
    "train_ds = ImageSegmentationDataset(parent_dir=training_data_folder, \n",
    "                                      image_processor=image_processor)\n",
    "\n",
    "testing_data_folder = \"/Users/briandunn/Desktop/Image Segmentation Projects/Carvana Image Masking/data\"\n",
    "\n",
    "test_ds = ImageSegmentationDataset(parent_dir=testing_data_folder, \n",
    "                                      image_processor=image_processor, \n",
    "                                      train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 4070\n",
      "Number of validation examples: 1018\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_ds))\n",
    "print(\"Number of validation examples:\", len(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape of Sample (Pixel Values Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = train_ds[12]\n",
    "\n",
    "sample['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape of Sample (Labels Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['labels'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Tensors Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.0434, 2.0434, 2.0434,  ..., 2.0605, 2.0605, 2.0605],\n",
       "         [2.0434, 2.0434, 2.0434,  ..., 2.0605, 2.0434, 2.0605],\n",
       "         [2.0434, 2.0434, 2.0434,  ..., 2.0434, 2.0434, 2.0434],\n",
       "         ...,\n",
       "         [0.9817, 0.9817, 0.9646,  ..., 0.9474, 0.9474, 0.9474],\n",
       "         [0.9646, 0.9646, 0.9646,  ..., 0.9303, 0.9474, 0.9474],\n",
       "         [0.9817, 0.9817, 0.9646,  ..., 0.9474, 0.9303, 0.9303]],\n",
       "\n",
       "        [[2.2185, 2.2185, 2.2185,  ..., 2.2360, 2.2360, 2.2360],\n",
       "         [2.2185, 2.2185, 2.2185,  ..., 2.2360, 2.2185, 2.2360],\n",
       "         [2.2185, 2.2185, 2.2185,  ..., 2.2185, 2.2185, 2.2185],\n",
       "         ...,\n",
       "         [1.2031, 1.2031, 1.1856,  ..., 1.1331, 1.1331, 1.1331],\n",
       "         [1.1856, 1.1856, 1.1856,  ..., 1.1155, 1.1331, 1.1331],\n",
       "         [1.2031, 1.2031, 1.1856,  ..., 1.1331, 1.1155, 1.1155]],\n",
       "\n",
       "        [[2.3960, 2.3960, 2.3960,  ..., 2.4483, 2.4483, 2.4483],\n",
       "         [2.3960, 2.3960, 2.3960,  ..., 2.4483, 2.4308, 2.4483],\n",
       "         [2.3960, 2.3960, 2.3960,  ..., 2.4308, 2.4308, 2.4308],\n",
       "         ...,\n",
       "         [1.4374, 1.4374, 1.4200,  ..., 1.3328, 1.3328, 1.3328],\n",
       "         [1.4200, 1.4200, 1.4200,  ..., 1.3154, 1.3328, 1.3328],\n",
       "         [1.4374, 1.4374, 1.4200,  ..., 1.3328, 1.3154, 1.3154]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['pixel_values']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique Label Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['labels'].squeeze().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Conversions Between String & Integer Values For Label Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Unique Label Values: ['background', 'vehicle']\n",
      "Number of Unique Label Values: 2\n",
      "label2id: {'background': 0, 'vehicle': 1}\n",
      "id2label: {0: 'background', 1: 'vehicle'}\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: \"background\", 1: \"vehicle\"}\n",
    "label2id = {label: idx for idx, label in id2label.items()}\n",
    "\n",
    "unqiue_label_values = list(id2label.values())\n",
    "\n",
    "NUM_OF_LABELS = len(unqiue_label_values)\n",
    "\n",
    "print(f\"List of Unique Label Values: {unqiue_label_values}\")\n",
    "print(f\"Number of Unique Label Values: {NUM_OF_LABELS}\")\n",
    "print(f\"label2id: {label2id}\")\n",
    "print(f\"id2label: {id2label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Compute Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"mean_iou\")\n",
    "    \n",
    "def compute_metrics(eval_pred):\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_pred\n",
    "        logits_tensor = torch.from_numpy(logits)\n",
    "        logits_tensor = nn.functional.interpolate(\n",
    "            logits_tensor,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).argmax(dim=1)\n",
    "\n",
    "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "        metrics = metric._compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=NUM_OF_LABELS,\n",
    "            ignore_index=255,\n",
    "            reduce_labels=False,\n",
    "        )\n",
    "        for key, value in metrics.items():\n",
    "            if type(value) is np.ndarray:\n",
    "                metrics[key] = value.tolist()\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Function to Display Number of Trainable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    This function prints the following values:\n",
    "    - the number of parameters\n",
    "    - the trainable parameters \n",
    "    - the percentage of total parameters \n",
    "        that are trainable.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"| Trainable parameters: {trainable_params} |\")\n",
    "    print(f\"| All parameters: {all_param} |\")\n",
    "    print(f\"| % Trainable: {100 * trainable_params / all_param:.2f}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nvidia/mit-b0 were not used when initializing SegformerForSemanticSegmentation: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.linear_c.3.proj.bias', 'decode_head.linear_fuse.weight', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.classifier.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.batch_norm.bias', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.1.proj.bias', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.classifier.weight', 'decode_head.batch_norm.weight', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Trainable parameters: 3714658 |\n",
      "| All parameters: 3714658 |\n",
      "| % Trainable: 100.00\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSemanticSegmentation.from_pretrained(\n",
    "    MODEL_CKPT, \n",
    "    id2label=id2label, \n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True)\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32, \n",
    "    lora_alpha=32,\n",
    "    target_modules=['query', 'values'],\n",
    "    lora_dropout=0.1,\n",
    "    bias='lora_only',\n",
    "    modules_to_save=['decode_head'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrap Base Model With LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Trainable parameters: 857092 |\n",
      "| All parameters: 4175460 |\n",
      "| % Trainable: 20.53\n"
     ]
    }
   ],
   "source": [
    "lora_model = get_peft_model(model, \n",
    "                            lora_config)\n",
    "\n",
    "print_trainable_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Early Stopping Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = [EarlyStoppingCallback(early_stopping_patience=4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    MODEL_NAME,\n",
    "    learning_rate=LR,\n",
    "    remove_unused_columns=False,\n",
    "    num_train_epochs=NUM_OF_EPOCHS,\n",
    "    auto_find_batch_size=True,\n",
    "    evaluation_strategy=STRATEGY,\n",
    "    eval_steps=509,\n",
    "    save_strategy=STRATEGY,\n",
    "    save_steps=509,\n",
    "    save_total_limit=6,\n",
    "    label_names=['labels'],\n",
    "    logging_first_step=True,\n",
    "    logging_strategy=STRATEGY,\n",
    "    logging_steps=51,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=REPORTS_TO,\n",
    "    hub_private_repo=True,\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/DunnBC22/mit-b0-Image_segmentation-Carvana_Image_Masking into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    lora_model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=early_stopping_callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briandunn/Desktop/python_venvs/torch_cv/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65bcec01ba5b4f5793e354de0e69742b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5090 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6576, 'learning_rate': 0.000499901768172888, 'epoch': 0.0}\n",
      "{'loss': 0.115, 'learning_rate': 0.0004949901768172888, 'epoch': 0.1}\n",
      "{'loss': 0.0314, 'learning_rate': 0.0004899803536345776, 'epoch': 0.2}\n",
      "{'loss': 0.0219, 'learning_rate': 0.0004849705304518664, 'epoch': 0.3}\n",
      "{'loss': 0.019, 'learning_rate': 0.00047996070726915517, 'epoch': 0.4}\n",
      "{'loss': 0.0171, 'learning_rate': 0.000474950884086444, 'epoch': 0.5}\n",
      "{'loss': 0.0155, 'learning_rate': 0.00046994106090373283, 'epoch': 0.6}\n",
      "{'loss': 0.0149, 'learning_rate': 0.00046493123772102166, 'epoch': 0.7}\n",
      "{'loss': 0.0149, 'learning_rate': 0.00045992141453831043, 'epoch': 0.8}\n",
      "{'loss': 0.0137, 'learning_rate': 0.0004549115913555992, 'epoch': 0.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5891eef439c49589c0a7517382f0670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9946260327775679, 0.9799209877962288]\" of type <class 'list'> for key \"eval/per_category_iou\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.9968763581592542, 0.99146628287315]\" of type <class 'list'> for key \"eval/per_category_accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.011340419761836529, 'eval_mean_iou': 0.9872735102868984, 'eval_mean_accuracy': 0.9941713205162022, 'eval_overall_accuracy': 0.9957426067419746, 'eval_per_category_iou': [0.9946260327775679, 0.9799209877962288], 'eval_per_category_accuracy': [0.9968763581592542, 0.99146628287315], 'eval_runtime': 262.5738, 'eval_samples_per_second': 3.877, 'eval_steps_per_second': 0.487, 'epoch': 1.0}\n",
      "{'loss': 0.0132, 'learning_rate': 0.00044990176817288804, 'epoch': 1.0}\n",
      "{'loss': 0.0128, 'learning_rate': 0.0004448919449901768, 'epoch': 1.1}\n",
      "{'loss': 0.0124, 'learning_rate': 0.00043988212180746564, 'epoch': 1.2}\n",
      "{'loss': 0.0123, 'learning_rate': 0.0004348722986247544, 'epoch': 1.3}\n",
      "{'loss': 0.0121, 'learning_rate': 0.0004298624754420432, 'epoch': 1.4}\n",
      "{'loss': 0.0113, 'learning_rate': 0.000424852652259332, 'epoch': 1.5}\n",
      "{'loss': 0.0111, 'learning_rate': 0.00041984282907662086, 'epoch': 1.6}\n",
      "{'loss': 0.0109, 'learning_rate': 0.00041483300589390963, 'epoch': 1.7}\n",
      "{'loss': 0.0108, 'learning_rate': 0.00040982318271119846, 'epoch': 1.8}\n",
      "{'loss': 0.011, 'learning_rate': 0.00040481335952848724, 'epoch': 1.9}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6bb32c53660435b95f19aaa7b69bfaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9953376607389561, 0.9825502616064588]\" of type <class 'list'> for key \"eval/per_category_iou\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.9973862234336488, 0.9922369645541501]\" of type <class 'list'> for key \"eval/per_category_accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.009576383978128433, 'eval_mean_iou': 0.9889439611727074, 'eval_mean_accuracy': 0.9948115939938995, 'eval_overall_accuracy': 0.9963071294758316, 'eval_per_category_iou': [0.9953376607389561, 0.9825502616064588], 'eval_per_category_accuracy': [0.9973862234336488, 0.9922369645541501], 'eval_runtime': 260.0824, 'eval_samples_per_second': 3.914, 'eval_steps_per_second': 0.492, 'epoch': 2.0}\n",
      "{'loss': 0.0111, 'learning_rate': 0.00039980353634577607, 'epoch': 2.0}\n",
      "{'loss': 0.0104, 'learning_rate': 0.00039479371316306484, 'epoch': 2.1}\n",
      "{'loss': 0.0105, 'learning_rate': 0.0003897838899803536, 'epoch': 2.2}\n",
      "{'loss': 0.0106, 'learning_rate': 0.00038477406679764245, 'epoch': 2.3}\n",
      "{'loss': 0.0102, 'learning_rate': 0.0003797642436149312, 'epoch': 2.4}\n",
      "{'loss': 0.0103, 'learning_rate': 0.00037475442043222, 'epoch': 2.5}\n",
      "{'loss': 0.0098, 'learning_rate': 0.0003697445972495089, 'epoch': 2.61}\n",
      "{'loss': 0.0095, 'learning_rate': 0.00036473477406679766, 'epoch': 2.71}\n",
      "{'loss': 0.0099, 'learning_rate': 0.0003597249508840865, 'epoch': 2.81}\n",
      "{'loss': 0.0096, 'learning_rate': 0.00035471512770137527, 'epoch': 2.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83acdb5820644958b85d8a8ea1d55fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9957651285733962, 0.9841233346724239]\" of type <class 'list'> for key \"eval/per_category_iou\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.9978333262127602, 0.9921659223657832]\" of type <class 'list'> for key \"eval/per_category_accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.008698945865035057, 'eval_mean_iou': 0.98994423162291, 'eval_mean_accuracy': 0.9949996242892717, 'eval_overall_accuracy': 0.9966456482593109, 'eval_per_category_iou': [0.9957651285733962, 0.9841233346724239], 'eval_per_category_accuracy': [0.9978333262127602, 0.9921659223657832], 'eval_runtime': 260.5438, 'eval_samples_per_second': 3.907, 'eval_steps_per_second': 0.491, 'epoch': 3.0}\n",
      "{'loss': 0.0098, 'learning_rate': 0.00034970530451866404, 'epoch': 3.01}\n",
      "{'loss': 0.0094, 'learning_rate': 0.00034469548133595287, 'epoch': 3.11}\n",
      "{'loss': 0.0093, 'learning_rate': 0.00033968565815324165, 'epoch': 3.21}\n",
      "{'loss': 0.0096, 'learning_rate': 0.0003346758349705304, 'epoch': 3.31}\n",
      "{'loss': 0.0095, 'learning_rate': 0.00032966601178781925, 'epoch': 3.41}\n",
      "{'loss': 0.0093, 'learning_rate': 0.00032465618860510803, 'epoch': 3.51}\n",
      "{'loss': 0.0098, 'learning_rate': 0.0003196463654223969, 'epoch': 3.61}\n",
      "{'loss': 0.0088, 'learning_rate': 0.0003146365422396857, 'epoch': 3.71}\n",
      "{'loss': 0.0088, 'learning_rate': 0.00030962671905697446, 'epoch': 3.81}\n",
      "{'loss': 0.0089, 'learning_rate': 0.0003046168958742633, 'epoch': 3.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4578eb82d554f6a9645b93183b1a15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.995939253870417, 0.9848038646083386]\" of type <class 'list'> for key \"eval/per_category_iou\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.9974856886435239, 0.9941433198992728]\" of type <class 'list'> for key \"eval/per_category_accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00818932056427002, 'eval_mean_iou': 0.9903715592393778, 'eval_mean_accuracy': 0.9958145042713984, 'eval_overall_accuracy': 0.9967852519396949, 'eval_per_category_iou': [0.995939253870417, 0.9848038646083386], 'eval_per_category_accuracy': [0.9974856886435239, 0.9941433198992728], 'eval_runtime': 263.3024, 'eval_samples_per_second': 3.866, 'eval_steps_per_second': 0.486, 'epoch': 4.0}\n",
      "{'loss': 0.0086, 'learning_rate': 0.00029960707269155207, 'epoch': 4.01}\n",
      "{'loss': 0.0088, 'learning_rate': 0.00029459724950884085, 'epoch': 4.11}\n",
      "{'loss': 0.0087, 'learning_rate': 0.0002895874263261297, 'epoch': 4.21}\n",
      "{'loss': 0.0086, 'learning_rate': 0.00028457760314341845, 'epoch': 4.31}\n",
      "{'loss': 0.0086, 'learning_rate': 0.0002795677799607073, 'epoch': 4.41}\n",
      "{'loss': 0.0088, 'learning_rate': 0.00027455795677799606, 'epoch': 4.51}\n",
      "{'loss': 0.0083, 'learning_rate': 0.0002695481335952849, 'epoch': 4.61}\n",
      "{'loss': 0.0085, 'learning_rate': 0.0002645383104125737, 'epoch': 4.71}\n",
      "{'loss': 0.0084, 'learning_rate': 0.0002595284872298625, 'epoch': 4.81}\n",
      "{'loss': 0.0086, 'learning_rate': 0.00025451866404715127, 'epoch': 4.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71bc8fe2e6794953b18d34062652bdad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9960739906614748, 0.985318731011033]\" of type <class 'list'> for key \"eval/per_category_iou\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.9973611355021557, 0.9951259658644163]\" of type <class 'list'> for key \"eval/per_category_accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.007810730487108231, 'eval_mean_iou': 0.9906963608362539, 'eval_mean_accuracy': 0.996243550683286, 'eval_overall_accuracy': 0.99689272672582, 'eval_per_category_iou': [0.9960739906614748, 0.985318731011033], 'eval_per_category_accuracy': [0.9973611355021557, 0.9951259658644163], 'eval_runtime': 263.2639, 'eval_samples_per_second': 3.867, 'eval_steps_per_second': 0.486, 'epoch': 5.0}\n",
      "{'loss': 0.0088, 'learning_rate': 0.0002495088408644401, 'epoch': 5.01}\n",
      "{'loss': 0.0084, 'learning_rate': 0.0002444990176817289, 'epoch': 5.11}\n",
      "{'loss': 0.0083, 'learning_rate': 0.00023948919449901768, 'epoch': 5.21}\n",
      "{'loss': 0.0086, 'learning_rate': 0.00023447937131630648, 'epoch': 5.31}\n",
      "{'loss': 0.0079, 'learning_rate': 0.0002294695481335953, 'epoch': 5.41}\n",
      "{'loss': 0.0082, 'learning_rate': 0.0002244597249508841, 'epoch': 5.51}\n",
      "{'loss': 0.0083, 'learning_rate': 0.0002194499017681729, 'epoch': 5.61}\n",
      "{'loss': 0.0079, 'learning_rate': 0.0002144400785854617, 'epoch': 5.71}\n",
      "{'loss': 0.0083, 'learning_rate': 0.0002094302554027505, 'epoch': 5.81}\n",
      "{'loss': 0.0082, 'learning_rate': 0.0002044204322200393, 'epoch': 5.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2828a8381fe146928bbd1a68781307f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.996111656579903, 0.9854654427500328]\" of type <class 'list'> for key \"eval/per_category_iou\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.9972840133423801, 0.9955608019924571]\" of type <class 'list'> for key \"eval/per_category_accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.007701446767896414, 'eval_mean_iou': 0.9907885496649679, 'eval_mean_accuracy': 0.9964224076674186, 'eval_overall_accuracy': 0.9969228920627436, 'eval_per_category_iou': [0.996111656579903, 0.9854654427500328], 'eval_per_category_accuracy': [0.9972840133423801, 0.9955608019924571], 'eval_runtime': 263.1956, 'eval_samples_per_second': 3.868, 'eval_steps_per_second': 0.486, 'epoch': 6.0}\n",
      "{'loss': 0.0083, 'learning_rate': 0.0001994106090373281, 'epoch': 6.01}\n",
      "{'loss': 0.0081, 'learning_rate': 0.0001944007858546169, 'epoch': 6.11}\n",
      "{'loss': 0.0078, 'learning_rate': 0.0001893909626719057, 'epoch': 6.21}\n",
      "{'loss': 0.0081, 'learning_rate': 0.00018438113948919448, 'epoch': 6.31}\n",
      "{'loss': 0.0079, 'learning_rate': 0.0001793713163064833, 'epoch': 6.41}\n",
      "{'loss': 0.0079, 'learning_rate': 0.00017436149312377212, 'epoch': 6.51}\n",
      "{'loss': 0.0077, 'learning_rate': 0.00016935166994106092, 'epoch': 6.61}\n",
      "{'loss': 0.0078, 'learning_rate': 0.0001643418467583497, 'epoch': 6.71}\n",
      "{'loss': 0.0078, 'learning_rate': 0.0001593320235756385, 'epoch': 6.81}\n",
      "{'loss': 0.0081, 'learning_rate': 0.00015432220039292733, 'epoch': 6.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08b5f95d7114610b4fd273ac7bb7072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9963687712634008, 0.986392918799812]\" of type <class 'list'> for key \"eval/per_category_iou\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.9978602677688214, 0.9943538179212852]\" of type <class 'list'> for key \"eval/per_category_accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.007229789160192013, 'eval_mean_iou': 0.9913808450316064, 'eval_mean_accuracy': 0.9961070428450534, 'eval_overall_accuracy': 0.9971254457425041, 'eval_per_category_iou': [0.9963687712634008, 0.986392918799812], 'eval_per_category_accuracy': [0.9978602677688214, 0.9943538179212852], 'eval_runtime': 263.5827, 'eval_samples_per_second': 3.862, 'eval_steps_per_second': 0.486, 'epoch': 7.0}\n",
      "{'loss': 0.0079, 'learning_rate': 0.00014931237721021613, 'epoch': 7.01}\n",
      "{'loss': 0.0081, 'learning_rate': 0.0001443025540275049, 'epoch': 7.11}\n",
      "{'loss': 0.008, 'learning_rate': 0.0001392927308447937, 'epoch': 7.21}\n",
      "{'loss': 0.008, 'learning_rate': 0.0001342829076620825, 'epoch': 7.31}\n",
      "{'loss': 0.0077, 'learning_rate': 0.00012927308447937131, 'epoch': 7.41}\n",
      "{'loss': 0.0074, 'learning_rate': 0.00012426326129666012, 'epoch': 7.51}\n",
      "{'loss': 0.0075, 'learning_rate': 0.00011925343811394892, 'epoch': 7.61}\n",
      "{'loss': 0.0075, 'learning_rate': 0.00011424361493123772, 'epoch': 7.72}\n",
      "{'loss': 0.0081, 'learning_rate': 0.00010923379174852653, 'epoch': 7.82}\n",
      "{'loss': 0.0081, 'learning_rate': 0.00010422396856581533, 'epoch': 7.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632b2c0796244a37996cdbb83233a19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9964252441704751, 0.9865990865839807]\" of type <class 'list'> for key \"eval/per_category_iou\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.9979579457243177, 0.9941981616792177]\" of type <class 'list'> for key \"eval/per_category_accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.007128467317670584, 'eval_mean_iou': 0.9915121653772279, 'eval_mean_accuracy': 0.9960780537017677, 'eval_overall_accuracy': 0.9971700342324488, 'eval_per_category_iou': [0.9964252441704751, 0.9865990865839807], 'eval_per_category_accuracy': [0.9979579457243177, 0.9941981616792177], 'eval_runtime': 279.6589, 'eval_samples_per_second': 3.64, 'eval_steps_per_second': 0.458, 'epoch': 8.0}\n",
      "{'loss': 0.0075, 'learning_rate': 9.921414538310412e-05, 'epoch': 8.02}\n",
      "{'loss': 0.0073, 'learning_rate': 9.420432220039293e-05, 'epoch': 8.12}\n",
      "{'loss': 0.0075, 'learning_rate': 8.919449901768172e-05, 'epoch': 8.22}\n",
      "{'loss': 0.0078, 'learning_rate': 8.418467583497053e-05, 'epoch': 8.32}\n",
      "{'loss': 0.0076, 'learning_rate': 7.917485265225933e-05, 'epoch': 8.42}\n",
      "{'loss': 0.0075, 'learning_rate': 7.416502946954813e-05, 'epoch': 8.52}\n",
      "{'loss': 0.0076, 'learning_rate': 6.915520628683693e-05, 'epoch': 8.62}\n",
      "{'loss': 0.0074, 'learning_rate': 6.414538310412574e-05, 'epoch': 8.72}\n",
      "{'loss': 0.0074, 'learning_rate': 5.913555992141454e-05, 'epoch': 8.82}\n",
      "{'loss': 0.0089, 'learning_rate': 5.412573673870334e-05, 'epoch': 8.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8cf9a9363b14613b43cfbfb25cbe348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9964702556912872, 0.9867634684594544]\" of type <class 'list'> for key \"eval/per_category_iou\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.9980368172126497, 0.9940702571301645]\" of type <class 'list'> for key \"eval/per_category_accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.007037504576146603, 'eval_mean_iou': 0.9916168620753708, 'eval_mean_accuracy': 0.9960535371714071, 'eval_overall_accuracy': 0.9972055731213163, 'eval_per_category_iou': [0.9964702556912872, 0.9867634684594544], 'eval_per_category_accuracy': [0.9980368172126497, 0.9940702571301645], 'eval_runtime': 264.4531, 'eval_samples_per_second': 3.849, 'eval_steps_per_second': 0.484, 'epoch': 9.0}\n",
      "{'loss': 0.0079, 'learning_rate': 4.911591355599214e-05, 'epoch': 9.02}\n",
      "{'loss': 0.0072, 'learning_rate': 4.410609037328094e-05, 'epoch': 9.12}\n",
      "{'loss': 0.0076, 'learning_rate': 3.9096267190569745e-05, 'epoch': 9.22}\n",
      "{'loss': 0.0087, 'learning_rate': 3.408644400785855e-05, 'epoch': 9.32}\n",
      "{'loss': 0.0077, 'learning_rate': 2.9076620825147347e-05, 'epoch': 9.42}\n",
      "{'loss': 0.0074, 'learning_rate': 2.406679764243615e-05, 'epoch': 9.52}\n",
      "{'loss': 0.0074, 'learning_rate': 1.905697445972495e-05, 'epoch': 9.62}\n",
      "{'loss': 0.0075, 'learning_rate': 1.4047151277013753e-05, 'epoch': 9.72}\n",
      "{'loss': 0.0075, 'learning_rate': 9.037328094302554e-06, 'epoch': 9.82}\n",
      "{'loss': 0.0076, 'learning_rate': 4.027504911591356e-06, 'epoch': 9.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6463d3d83054c9c9d4a411f3df12066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"[0.9964996655500316, 0.9868763925617403]\" of type <class 'list'> for key \"eval/per_category_iou\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"[0.9980006976075766, 0.994318466698934]\" of type <class 'list'> for key \"eval/per_category_accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00696456478908658, 'eval_mean_iou': 0.9916880290558859, 'eval_mean_accuracy': 0.9961595821532553, 'eval_overall_accuracy': 0.9972290383809208, 'eval_per_category_iou': [0.9964996655500316, 0.9868763925617403], 'eval_per_category_accuracy': [0.9980006976075766, 0.994318466698934], 'eval_runtime': 264.186, 'eval_samples_per_second': 3.853, 'eval_steps_per_second': 0.485, 'epoch': 10.0}\n",
      "{'train_runtime': 29191.0045, 'train_samples_per_second': 1.394, 'train_steps_per_second': 0.174, 'train_loss': 0.010749788351049592, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5090, training_loss=0.010749788351049592, metrics={'train_runtime': 29191.0045, 'train_samples_per_second': 1.394, 'train_steps_per_second': 0.174, 'train_loss': 0.010749788351049592, 'epoch': 10.0})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(f'{MODEL_NAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Push Model to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (2) will be pushed upstream.\n",
      "The progress bars may be unreliable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fcf3afa4cb4291b4dbc6dcb7dabd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/16.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7db10c63ee4257b277e428b344fede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file adapter_model.bin:   0%|          | 1.00/1.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21cfa924c844873b37f2ab374064e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Jul31_22-37-29_Brians-Mac-mini/events.out.tfevents.1690861054.Brians-Mac-mini.7948.2:   0%|  â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/DunnBC22/mit-b0-Image_segmentation-Carvana_Image_Masking\n",
      "   2159b20..0193be1  main -> main\n",
      "\n",
      "To https://huggingface.co/DunnBC22/mit-b0-Image_segmentation-Carvana_Image_Masking\n",
      "   0193be1..b70b818  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/DunnBC22/mit-b0-Image_segmentation-Carvana_Image_Masking/commit/0193be1ddbd4f50fba521581127794c93c072792'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(\"All DUNN!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes & Other Takeaways From This Project\n",
    "\n",
    "****\n",
    "- I am surprised at both how well the results are of this project and how quickly this project converged on the optimal settings. \n",
    "\n",
    "- Originally, I ran this project for 50 epochs (but due to unfortunate circumstances, it timed out before finishing), so I trained it for 10 epochs instead. The metrics for this model versus the 50 epoch model are not much different.\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "\n",
    "- Model Checkpoint (nvidia/mit-b0)\n",
    "    > @article{DBLP:journals/corr/abs-2105-15203, author = {Enze Xie and Wenhai Wang and Zhiding Yu and Anima Anandkumar and Jose M. Alvarez and Ping Luo}, title = {SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers}, journal = {CoRR}, volume = {abs/2105.15203}, year = {2021}, url = {https://arxiv.org/abs/2105.15203}, eprinttype = {arXiv}, eprint = {2105.15203}, timestamp = {Wed, 02 Jun 2021 11:46:42 +0200}, biburl = {https://dblp.org/rec/journals/corr/abs-2105-15203.bib}, bibsource = {dblp computer science bibliography, https://dblp.org}}\n",
    "\n",
    "- Metrics (mean_iou)\n",
    "    > @software{MMSegmentation_Contributors_OpenMMLab_Semantic_Segmentation_2020, author = {{MMSegmentation Contributors}}, license = {Apache-2.0}, month = {7}, title = {{OpenMMLab Semantic Segmentation Toolbox and Benchmark}}, url = {https://github.com/open-mmlab/mmsegmentation}, year = {2020}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
